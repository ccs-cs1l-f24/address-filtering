{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Social Network\n",
    "We can construct a graph $G = (V,E)$ where $V$ is a set of Ethereum addresses that have completed at least one transaction. We can then label these addresses (which may or may not be null). Then, $E$ is the set of arcs where $(n_i, n_j, TrS_{ij}) \\in E$ if there is at least one transaction from $n_i$ to $n_j$. Note that $TrS_{ij}$ consists of a set of triplets $(tr_{ij_k}, \\tau_{ij_k}, v_{ij_k})$ where $tr_{ij_k}$ is the $k^{th}$ transaction from $n_i$ to $n_j$, $\\tau_{ij_k}$ is the timestamp, and $v_{ij_k}$ is the amount of Wei transferred.\n",
    "\n",
    "Then, we can select a few factors from Social Network Analysis to characterize each address:\n",
    "1. In degree\n",
    "2. Out degree\n",
    "3. In transaction\n",
    "4. Out transaction\n",
    "5. In value\n",
    "6. Out value\n",
    "7. Clustering coefficient\n",
    "8. PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eros Distance (Extended Frobenius Norm)\n",
    "\n",
    "Let $A$ and $B$ be two multivariate time series of size $m_A \\times n$ and $m_B \\times n$ respectively, where $m_A, m_B$ are the number of observations and $n$ is the number of factors. Then, construct the covariance matrices and denote these as $M_A, M_B$. Then, apply SVD to these matrices to construct the right eigenvectors matrices $V_A, V_B$. Let $V_A = [a_1, \\dots, a_n]$ and $V_B = [b_1, \\dots, b_n]$, where $a_i,b_i$ are column orthonormal vectors of size $n$. Then, $$\\text{Eros} (A,B,w) = \\sum_{i=1}^n w_i | \\langle a_i, b_i \\rangle | = \\sum_{i=1}^n w_i | \\cos \\theta_i |$$\n",
    "Note that the distance will be a value on the interval $[0,1]$, where $1$ is the most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b) / (npla.norm(a) * npla.norm(b))\n",
    "\n",
    "def Eros(A, B, weights):\n",
    "    n = np.shape(A)[1]\n",
    "\n",
    "    cov_A = np.cov(A.T)\n",
    "    cov_B = np.cov(B.T)\n",
    "\n",
    "    U_A, S_A, V_A = npla.svd(cov_A)\n",
    "    U_B, S_B, V_B = npla.svd(cov_B)\n",
    "\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        result += weights[i] * np.abs(cosine_similarity(V_A[i], V_B[i]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grabbed some data about a subset of the exchange addresses to construct a basic multivariate time series. I have not yet constructed a network so the features related to that have been omitted for now. Hence, I just need to load these values into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = []\n",
    "for file in os.listdir('Code-Files/time_series_data'):\n",
    "    f = os.path.join('Code-Files/time_series_data', file)\n",
    "    arr = np.genfromtxt(f, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "    arr = np.array([[int(x.decode()) if isinstance(x, bytes) else x for x in row] for row in arr])    \n",
    "    arrays.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the optimal weights for the system since just using variations of the eigenvalues does not necessarily capture the traits of interest in our system. I will be using a practice dataset for this training, which contains a bunch addresses I do not want to include in my data. To start, I just focused on exchanges since I already had a very basic list of some known exchanges. Furthermore, rather than testing every permutation, I am going to do a few random weights and find the best (for now, at least)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11231884 0.22826087 0.15217391 0.1884058  0.14492754 0.17391304]\n",
      "[0.11842105 0.14802632 0.24013158 0.0625     0.14144737 0.28947368]\n",
      "[0.30147059 0.11029412 0.3125     0.02573529 0.10294118 0.14705882]\n",
      "[0.05625  0.278125 0.2875   0.       0.134375 0.24375 ]\n",
      "[0.25514403 0.06995885 0.3744856  0.01646091 0.09465021 0.18930041]\n",
      "[0.23834197 0.07253886 0.37305699 0.01036269 0.07253886 0.23316062]\n",
      "[0.2375  0.075   0.425   0.      0.03125 0.23125]\n",
      "[0.02830189 0.         0.67924528 0.03773585 0.11320755 0.14150943]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(optimal_weight)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimal_weight\n\u001b[0;32m---> 33\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mfind_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(opt)\n",
      "Cell \u001b[0;32mIn[83], line 19\u001b[0m, in \u001b[0;36mfind_weights\u001b[0;34m(test_data, weights)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(sample_2 \u001b[38;5;241m==\u001b[39m sample_1):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mEros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     22\u001b[0m avg_diff \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d\n",
      "Cell \u001b[0;32mIn[81], line 7\u001b[0m, in \u001b[0;36mEros\u001b[0;34m(A, B, weights)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mEros\u001b[39m(A, B, weights):\n\u001b[1;32m      5\u001b[0m     n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(A)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m     cov_A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     cov_B \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(B\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     10\u001b[0m     U_A, S_A, V_A \u001b[38;5;241m=\u001b[39m npla\u001b[38;5;241m.\u001b[39msvd(cov_A)\n",
      "File \u001b[0;32m~/Desktop/ethereum-clustering/.venv/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:2870\u001b[0m, in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2867\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2868\u001b[0m         w \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m aweights\n\u001b[0;32m-> 2870\u001b[0m avg, w_sum \u001b[38;5;241m=\u001b[39m \u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2871\u001b[0m w_sum \u001b[38;5;241m=\u001b[39m w_sum[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;66;03m# Determine the normalization\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ethereum-clustering/.venv/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:552\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    549\u001b[0m     keepdims_kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims}\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeepdims_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(avg)\n\u001b[1;32m    554\u001b[0m     scl \u001b[38;5;241m=\u001b[39m avg_as_array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(a\u001b[38;5;241m.\u001b[39msize\u001b[38;5;241m/\u001b[39mavg_as_array\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[0;32m~/Desktop/ethereum-clustering/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:123\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    119\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    121\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m rcount \u001b[38;5;241m=\u001b[39m \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    125\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ethereum-clustering/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:82\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(arr\u001b[38;5;241m.\u001b[39mndim))\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     83\u001b[0m     axis \u001b[38;5;241m=\u001b[39m (axis,)\n\u001b[1;32m     84\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "\n",
    "for i in range(100000):\n",
    "    r = np.random.randint(100,size=(6))\n",
    "    r = r / np.sum(r)\n",
    "    weights.append(r)\n",
    "\n",
    "def find_weights(test_data, weights):\n",
    "    min_diff = 2**31\n",
    "    optimal_weight = []\n",
    "\n",
    "    for w in weights:\n",
    "        avg_diff = 0\n",
    "        val = 0\n",
    "        for sample_1 in test_data:\n",
    "            for sample_2 in test_data: \n",
    "                if np.all(sample_2 == sample_1):\n",
    "                    continue\n",
    "                d = Eros(sample_1, sample_2, w)\n",
    "                \n",
    "                val += 1\n",
    "                avg_diff += d\n",
    "\n",
    "        if avg_diff != 0 and val != 0:\n",
    "            avg_diff = avg_diff / val\n",
    "            if avg_diff < min_diff:\n",
    "                min_diff = avg_diff\n",
    "                optimal_weight = w\n",
    "                print(optimal_weight)\n",
    "    \n",
    "    return optimal_weight\n",
    "\n",
    "opt = find_weights(arrays, weights)\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Now, I have determined an optimal weight for classifying an address as an exchange address (well, sort of). This can then be used to find the Eros distance between a new address and a known exchange address. Then, I can define some threshold value for classification.\n",
    "\n",
    "In this case, matrix $A$,$B$ are exchanges classified as part of the same class, meanwhile $C$ is just a random address I pulled from Etherscan (its had very different behavior). Thus, we see the distance calculation provides us with the results we would want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eros between two exchanges: 0.9997584716687287\n",
      "Eros between exchange and random address: 0.11320504923754138\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0.02830189, 0, 0.67924528, 0.03773585, 0.11320755, 0.14150943])\n",
    "\n",
    "f = 'Code-Files/time_series_data/series_0xd433138d12beB9929FF6fd583DC83663eea6Aaa5.csv'\n",
    "A = np.genfromtxt(f, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "A = np.array([[int(x.decode()) if isinstance(x, bytes) else x for x in row] for row in A]) \n",
    "\n",
    "f = 'Code-Files/time_series_data/series_0x9B99CcA871Be05119B2012fd4474731dd653FEBe.csv'\n",
    "B = np.genfromtxt(f, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "B = np.array([[int(x.decode()) if isinstance(x, bytes) else x for x in row] for row in B]) \n",
    "\n",
    "\n",
    "f = 'Code-Files/time_series_data/series_0x4838B106FCe9647Bdf1E7877BF73cE8B0BAD5f97.csv'\n",
    "C = np.genfromtxt(f, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "C = np.array([[int(x.decode()) if isinstance(x, bytes) else x for x in row] for row in C]) \n",
    "\n",
    "print(\"Eros between two exchanges:\", Eros(A,B,weights))\n",
    "print(\"Eros between exchange and random address:\", Eros(A,C,weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
